{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-1.4.2-py2.py3-none-any.whl (4.2 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-1.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init(\"/Users/seneralkan/spark/spark-3.1.2-bin-hadoop3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Spark App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"pyspark_101\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark_101</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=pyspark_101>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.2'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pyspark_101'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.appName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic DataFrame Processes\n",
    "\n",
    "Spark Dataframe is diffrent rather than pandas dataframe\n",
    "\n",
    "Check the examples below !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.read.csv('./churn.csv', header = True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: int, Names: string, Age: double, Total_Purchase: double, Account_Manager: int, Years: double, Num_Sites: double, Churn: int]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df=sns.load_dataset(\"diamonds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat      cut color clarity  depth  table  price     x     y     z\n",
       "0   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n",
       "1   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n",
       "2   0.23     Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n",
       "3   0.29  Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n",
       "4   0.31     Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_c0=0, Names='Cameron Williams', Age=42.0, Total_Purchase=11066.8, Account_Manager=0, Years=7.22, Num_Sites=8.0, Churn=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "carat       float64\n",
       "cut        category\n",
       "color      category\n",
       "clarity    category\n",
       "depth       float64\n",
       "table       float64\n",
       "price         int64\n",
       "x           float64\n",
       "y           float64\n",
       "z           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'int'),\n",
       " ('Names', 'string'),\n",
       " ('Age', 'double'),\n",
       " ('Total_Purchase', 'double'),\n",
       " ('Account_Manager', 'int'),\n",
       " ('Years', 'double'),\n",
       " ('Num_Sites', 'double'),\n",
       " ('Churn', 'int')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-1d8d3a0cd3e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1641\u001b[0m         \"\"\"\n\u001b[1;32m   1642\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1643\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   1644\u001b[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[1;32m   1645\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "spark_df.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----+--------------+---------------+-----+---------+-----+\n",
      "|_c0|           Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|Churn|\n",
      "+---+----------------+----+--------------+---------------+-----+---------+-----+\n",
      "|  0|Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|    1|\n",
      "|  1|   Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|    1|\n",
      "|  2|     Eric Lozano|38.0|      12884.75|              0| 6.67|     12.0|    1|\n",
      "+---+----------------+----+--------------+---------------+-----+---------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparf_df.show(3, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " 'Names',\n",
       " 'Age',\n",
       " 'Total_Purchase',\n",
       " 'Account_Manager',\n",
       " 'Years',\n",
       " 'Num_Sites',\n",
       " 'Churn']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------+-----------------+-----------------+------------------+-----------------+------------------+-------------------+\n",
      "|summary|               _c0|        Names|              Age|   Total_Purchase|   Account_Manager|            Years|         Num_Sites|              Churn|\n",
      "+-------+------------------+-------------+-----------------+-----------------+------------------+-----------------+------------------+-------------------+\n",
      "|  count|               900|          900|              900|              900|               900|              900|               900|                900|\n",
      "|   mean|             449.5|         null|41.81666666666667|10062.82403333334|0.4811111111111111| 5.27315555555555| 8.587777777777777|0.16666666666666666|\n",
      "| stddev|259.95191863111916|         null|6.127560416916251|2408.644531858096|0.4999208935073339|1.274449013194616|1.7648355920350969| 0.3728852122772358|\n",
      "|    min|                 0|   Aaron King|             22.0|            100.0|                 0|              1.0|               3.0|                  0|\n",
      "|    max|               899|Zachary Walsh|             65.0|         18026.01|                 1|             9.15|              14.0|                  1|\n",
      "+-------+------------------+-------------+-----------------+-----------------+------------------+-----------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|              Age|\n",
      "+-------+-----------------+\n",
      "|  count|              900|\n",
      "|   mean|41.81666666666667|\n",
      "| stddev|6.127560416916251|\n",
      "|    min|             22.0|\n",
      "|    max|             65.0|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.describe(\"Age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+\n",
      "| Age|              Names|\n",
      "+----+-------------------+\n",
      "|42.0|   Cameron Williams|\n",
      "|41.0|      Kevin Mueller|\n",
      "|38.0|        Eric Lozano|\n",
      "|42.0|      Phillip White|\n",
      "|37.0|     Cynthia Norton|\n",
      "|48.0|   Jessica Williams|\n",
      "|44.0|        Eric Butler|\n",
      "|32.0|      Zachary Walsh|\n",
      "|43.0|        Ashlee Carr|\n",
      "|40.0|     Jennifer Lynch|\n",
      "|30.0|       Paula Harris|\n",
      "|45.0|     Bruce Phillips|\n",
      "|45.0|       Craig Garner|\n",
      "|40.0|       Nicole Olson|\n",
      "|41.0|     Harold Griffin|\n",
      "|38.0|       James Wright|\n",
      "|45.0|      Doris Wilkins|\n",
      "|43.0|Katherine Carpenter|\n",
      "|53.0|     Lindsay Martin|\n",
      "|46.0|        Kathy Curry|\n",
      "+----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.select(\"Age\", \"Names\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.filter(spark_df.Age > 40).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Churn|count|\n",
      "+-----+-----+\n",
      "|    1|  150|\n",
      "|    0|  750|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.groupby(\"Churn\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|Churn|         avg(Age)|\n",
      "+-----+-----------------+\n",
      "|    1|42.99333333333333|\n",
      "|    0|41.58133333333333|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.groupby(\"Churn\").agg({\"Age\": \"mean\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.createOrReplaceTempView(\"tbl_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|        |   tbl_df|       true|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| Age|\n",
      "+----+\n",
      "|42.0|\n",
      "|41.0|\n",
      "|38.0|\n",
      "|42.0|\n",
      "|37.0|\n",
      "|48.0|\n",
      "|44.0|\n",
      "|32.0|\n",
      "|43.0|\n",
      "|40.0|\n",
      "|30.0|\n",
      "|45.0|\n",
      "|45.0|\n",
      "|40.0|\n",
      "|41.0|\n",
      "|38.0|\n",
      "|45.0|\n",
      "|43.0|\n",
      "|53.0|\n",
      "|46.0|\n",
      "+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Age from tbl_df\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|Churn|        mean(Age)|\n",
      "+-----+-----------------+\n",
      "|    1|42.99333333333333|\n",
      "|    0|41.58133333333333|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Churn, mean(Age) from tbl_df group by Churn\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualization Big Data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Churn'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## First, we need to reduce the spark df to pandas df\n",
    "sdf=spark_df.toPandas()\n",
    "sns.barplot(x=\"Churn\", y=sdf.Churn.index, data=sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+----+--------------+---------------+-----+---------+-----+\n",
      "|_c0|              Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|Churn|\n",
      "+---+-------------------+----+--------------+---------------+-----+---------+-----+\n",
      "|  0|   Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|    1|\n",
      "|  1|      Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|    1|\n",
      "|  2|        Eric Lozano|38.0|      12884.75|              0| 6.67|     12.0|    1|\n",
      "|  3|      Phillip White|42.0|       8010.76|              0| 6.71|     10.0|    1|\n",
      "|  4|     Cynthia Norton|37.0|       9191.58|              0| 5.56|      9.0|    1|\n",
      "|  5|   Jessica Williams|48.0|      10356.02|              0| 5.12|      8.0|    1|\n",
      "|  6|        Eric Butler|44.0|      11331.58|              1| 5.23|     11.0|    1|\n",
      "|  7|      Zachary Walsh|32.0|       9885.12|              1| 6.92|      9.0|    1|\n",
      "|  8|        Ashlee Carr|43.0|       14062.6|              1| 5.46|     11.0|    1|\n",
      "|  9|     Jennifer Lynch|40.0|       8066.94|              1| 7.11|     11.0|    1|\n",
      "| 10|       Paula Harris|30.0|      11575.37|              1| 5.22|      8.0|    1|\n",
      "| 11|     Bruce Phillips|45.0|       8771.02|              1| 6.64|     11.0|    1|\n",
      "| 12|       Craig Garner|45.0|       8988.67|              1| 4.84|     11.0|    1|\n",
      "| 13|       Nicole Olson|40.0|       8283.32|              1|  5.1|     13.0|    1|\n",
      "| 14|     Harold Griffin|41.0|       6569.87|              1|  4.3|     11.0|    1|\n",
      "| 15|       James Wright|38.0|      10494.82|              1| 6.81|     12.0|    1|\n",
      "| 16|      Doris Wilkins|45.0|       8213.41|              1| 7.35|     11.0|    1|\n",
      "| 17|Katherine Carpenter|43.0|      11226.88|              0| 8.08|     12.0|    1|\n",
      "| 18|     Lindsay Martin|53.0|       5515.09|              0| 6.85|      8.0|    1|\n",
      "| 19|        Kathy Curry|46.0|        8046.4|              1| 5.69|      8.0|    1|\n",
      "+---+-------------------+----+--------------+---------------+-----+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.toDF(*[c.lower() for c in spark_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+----+--------------+---------------+-----+---------+-----+\n",
      "|_c0|              names| age|total_purchase|account_manager|years|num_sites|churn|\n",
      "+---+-------------------+----+--------------+---------------+-----+---------+-----+\n",
      "|  0|   Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|    1|\n",
      "|  1|      Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|    1|\n",
      "|  2|        Eric Lozano|38.0|      12884.75|              0| 6.67|     12.0|    1|\n",
      "|  3|      Phillip White|42.0|       8010.76|              0| 6.71|     10.0|    1|\n",
      "|  4|     Cynthia Norton|37.0|       9191.58|              0| 5.56|      9.0|    1|\n",
      "|  5|   Jessica Williams|48.0|      10356.02|              0| 5.12|      8.0|    1|\n",
      "|  6|        Eric Butler|44.0|      11331.58|              1| 5.23|     11.0|    1|\n",
      "|  7|      Zachary Walsh|32.0|       9885.12|              1| 6.92|      9.0|    1|\n",
      "|  8|        Ashlee Carr|43.0|       14062.6|              1| 5.46|     11.0|    1|\n",
      "|  9|     Jennifer Lynch|40.0|       8066.94|              1| 7.11|     11.0|    1|\n",
      "| 10|       Paula Harris|30.0|      11575.37|              1| 5.22|      8.0|    1|\n",
      "| 11|     Bruce Phillips|45.0|       8771.02|              1| 6.64|     11.0|    1|\n",
      "| 12|       Craig Garner|45.0|       8988.67|              1| 4.84|     11.0|    1|\n",
      "| 13|       Nicole Olson|40.0|       8283.32|              1|  5.1|     13.0|    1|\n",
      "| 14|     Harold Griffin|41.0|       6569.87|              1|  4.3|     11.0|    1|\n",
      "| 15|       James Wright|38.0|      10494.82|              1| 6.81|     12.0|    1|\n",
      "| 16|      Doris Wilkins|45.0|       8213.41|              1| 7.35|     11.0|    1|\n",
      "| 17|Katherine Carpenter|43.0|      11226.88|              0| 8.08|     12.0|    1|\n",
      "| 18|     Lindsay Martin|53.0|       5515.09|              0| 6.85|      8.0|    1|\n",
      "| 19|        Kathy Curry|46.0|        8046.4|              1| 5.69|      8.0|    1|\n",
      "+---+-------------------+----+--------------+---------------+-----+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the columns name\n",
    "spark_df = spark_df.withColumnRenamed(\"_c0\", \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['index',\n",
       " 'names',\n",
       " 'age',\n",
       " 'total_purchase',\n",
       " 'account_manager',\n",
       " 'years',\n",
       " 'num_sites',\n",
       " 'churn']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------+-----------------+-----------------+------------------+-----------------+------------------+-------------------+\n",
      "|summary|             index|        names|              age|   total_purchase|   account_manager|            years|         num_sites|              churn|\n",
      "+-------+------------------+-------------+-----------------+-----------------+------------------+-----------------+------------------+-------------------+\n",
      "|  count|               900|          900|              900|              900|               900|              900|               900|                900|\n",
      "|   mean|             449.5|         null|41.81666666666667|10062.82403333334|0.4811111111111111| 5.27315555555555| 8.587777777777777|0.16666666666666666|\n",
      "| stddev|259.95191863111916|         null|6.127560416916251|2408.644531858096|0.4999208935073339|1.274449013194616|1.7648355920350969| 0.3728852122772358|\n",
      "|    min|                 0|   Aaron King|             22.0|            100.0|                 0|              1.0|               3.0|                  0|\n",
      "|    max|               899|Zachary Walsh|             65.0|         18026.01|                 1|             9.15|              14.0|                  1|\n",
      "+-------+------------------+-------------+-----------------+-----------------+------------------+-----------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>900</td>\n",
       "      <td>41.81666666666667</td>\n",
       "      <td>6.127560416916251</td>\n",
       "      <td>22.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_purchase</th>\n",
       "      <td>900</td>\n",
       "      <td>10062.82403333334</td>\n",
       "      <td>2408.644531858096</td>\n",
       "      <td>100.0</td>\n",
       "      <td>18026.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>account_manager</th>\n",
       "      <td>900</td>\n",
       "      <td>0.4811111111111111</td>\n",
       "      <td>0.4999208935073339</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>churn</th>\n",
       "      <td>900</td>\n",
       "      <td>0.16666666666666666</td>\n",
       "      <td>0.3728852122772358</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0                    1                   2      3  \\\n",
       "summary          count                 mean              stddev    min   \n",
       "age                900    41.81666666666667   6.127560416916251   22.0   \n",
       "total_purchase     900    10062.82403333334   2408.644531858096  100.0   \n",
       "account_manager    900   0.4811111111111111  0.4999208935073339      0   \n",
       "churn              900  0.16666666666666666  0.3728852122772358      0   \n",
       "\n",
       "                        4  \n",
       "summary               max  \n",
       "age                  65.0  \n",
       "total_purchase   18026.01  \n",
       "account_manager         1  \n",
       "churn                   1  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.select(\"age\", \"total_purchase\", \"account_manager\", \"churn\").describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pre-processing\n",
    "# Drop missing values\n",
    "spark_df = spark_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+----+--------------+---------------+-----+---------+-----+------+------+\n",
      "|index|              names| age|total_purchase|account_manager|years|num_sites|churn| age_2| age^2|\n",
      "+-----+-------------------+----+--------------+---------------+-----+---------+-----+------+------+\n",
      "|    0|   Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|    1|1764.0|1764.0|\n",
      "|    1|      Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|    1|1681.0|1681.0|\n",
      "|    2|        Eric Lozano|38.0|      12884.75|              0| 6.67|     12.0|    1|1444.0|1444.0|\n",
      "|    3|      Phillip White|42.0|       8010.76|              0| 6.71|     10.0|    1|1764.0|1764.0|\n",
      "|    4|     Cynthia Norton|37.0|       9191.58|              0| 5.56|      9.0|    1|1369.0|1369.0|\n",
      "|    5|   Jessica Williams|48.0|      10356.02|              0| 5.12|      8.0|    1|2304.0|2304.0|\n",
      "|    6|        Eric Butler|44.0|      11331.58|              1| 5.23|     11.0|    1|1936.0|1936.0|\n",
      "|    7|      Zachary Walsh|32.0|       9885.12|              1| 6.92|      9.0|    1|1024.0|1024.0|\n",
      "|    8|        Ashlee Carr|43.0|       14062.6|              1| 5.46|     11.0|    1|1849.0|1849.0|\n",
      "|    9|     Jennifer Lynch|40.0|       8066.94|              1| 7.11|     11.0|    1|1600.0|1600.0|\n",
      "|   10|       Paula Harris|30.0|      11575.37|              1| 5.22|      8.0|    1| 900.0| 900.0|\n",
      "|   11|     Bruce Phillips|45.0|       8771.02|              1| 6.64|     11.0|    1|2025.0|2025.0|\n",
      "|   12|       Craig Garner|45.0|       8988.67|              1| 4.84|     11.0|    1|2025.0|2025.0|\n",
      "|   13|       Nicole Olson|40.0|       8283.32|              1|  5.1|     13.0|    1|1600.0|1600.0|\n",
      "|   14|     Harold Griffin|41.0|       6569.87|              1|  4.3|     11.0|    1|1681.0|1681.0|\n",
      "|   15|       James Wright|38.0|      10494.82|              1| 6.81|     12.0|    1|1444.0|1444.0|\n",
      "|   16|      Doris Wilkins|45.0|       8213.41|              1| 7.35|     11.0|    1|2025.0|2025.0|\n",
      "|   17|Katherine Carpenter|43.0|      11226.88|              0| 8.08|     12.0|    1|1849.0|1849.0|\n",
      "|   18|     Lindsay Martin|53.0|       5515.09|              0| 6.85|      8.0|    1|2809.0|2809.0|\n",
      "|   19|        Kathy Curry|46.0|        8046.4|              1| 5.69|      8.0|    1|2116.0|2116.0|\n",
      "+-----+-------------------+----+--------------+---------------+-----+---------+-----+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df = spark_df.withColumn(\"age^2\", spark_df.age **2)\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining the dependent variable\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "# We need to assign the churn column as label for ml processes\n",
    "stringIndexer = StringIndexer(inputCol =\"churn\", outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Output column label already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-c60f61354666>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstringIndexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mindexed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Changing the label data type to integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mspark_df\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mindexed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"integer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Output column label already exists."
     ]
    }
   ],
   "source": [
    "model = stringIndexer.fit(spark_df)\n",
    "indexed = model.transform(spark_df)\n",
    "\n",
    "# Changing the label data type to integer\n",
    "spark_df =indexed.withColumn(\"label\", indexed[\"label\"].cast(\"integer\"))\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining independent variable\n",
    "independent_var =[\"age\", \"total_purchase\", \"account_manager\", \"years\", \"num_sites\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=independent_var, outputCol=\"features\")\n",
    "\n",
    "feature_df = vectorAssembler.transform(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = feature_df.select([\"features\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[42.0,11066.8,0.0...|    1|\n",
      "|[41.0,11916.22,0....|    1|\n",
      "|[38.0,12884.75,0....|    1|\n",
      "|[42.0,8010.76,0.0...|    1|\n",
      "|[37.0,9191.58,0.0...|    1|\n",
      "|[48.0,10356.02,0....|    1|\n",
      "|[44.0,11331.58,1....|    1|\n",
      "|[32.0,9885.12,1.0...|    1|\n",
      "|[43.0,14062.6,1.0...|    1|\n",
      "|[40.0,8066.94,1.0...|    1|\n",
      "|[30.0,11575.37,1....|    1|\n",
      "|[45.0,8771.02,1.0...|    1|\n",
      "|[45.0,8988.67,1.0...|    1|\n",
      "|[40.0,8283.32,1.0...|    1|\n",
      "|[41.0,6569.87,1.0...|    1|\n",
      "|[38.0,10494.82,1....|    1|\n",
      "|[45.0,8213.41,1.0...|    1|\n",
      "|[43.0,11226.88,0....|    1|\n",
      "|[53.0,5515.09,0.0...|    1|\n",
      "|[46.0,8046.4,1.0,...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test\n",
    "splits =final_df.randomSplit([0.70, 0.30])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Churn Analysis - ML Modelling\n",
    "\n",
    "Gradient-Boosted Trees (GBTs) is a learning algorithm for classification. It supports binary labels, as well as both continuous and categorical features. Note: Multiclass labels are not currently supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = GBTClassifier(maxIter=10, featuresCol= \"features\", labelCol= \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "gbm_model = gbm.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gbm_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[22.0,11254.38,1....|    0|[0.81944759801368...|[0.83738455049384...|       0.0|\n",
      "|[26.0,8939.61,0.0...|    0|[1.37441121280323...|[0.93984681038467...|       0.0|\n",
      "|[27.0,8628.8,1.0,...|    0|[1.02273454096283...|[0.88548899344856...|       0.0|\n",
      "|[28.0,9090.43,1.0...|    0|[-1.1182760211156...|[0.09651578731992...|       1.0|\n",
      "|[29.0,9378.24,0.0...|    0|[1.37106532333162...|[0.93946737671853...|       0.0|\n",
      "|[29.0,13240.01,1....|    0|[0.39667442031882...|[0.68854993891737...|       0.0|\n",
      "|[29.0,13255.05,1....|    0|[0.17110630862498...|[0.58472789379771...|       0.0|\n",
      "|[30.0,8403.78,1.0...|    0|[1.09339557061350...|[0.89905706407921...|       0.0|\n",
      "|[30.0,8677.28,1.0...|    0|[1.06474025359877...|[0.89373568103414...|       0.0|\n",
      "|[30.0,12788.37,0....|    0|[0.98132288042982...|[0.87681899931086...|       0.0|\n",
      "|[30.0,13473.35,0....|    0|[1.07386146260694...|[0.89545580105960...|       0.0|\n",
      "|[31.0,8688.21,0.0...|    0|[1.32947717922590...|[0.93456074736976...|       0.0|\n",
      "|[31.0,11297.57,1....|    1|[-0.0251975584178...|[0.48740388650689...|       1.0|\n",
      "|[32.0,5756.12,0.0...|    0|[0.98529337771239...|[0.87767412252497...|       0.0|\n",
      "|[32.0,11540.86,0....|    0|[1.32128726051755...|[0.93355184753314...|       0.0|\n",
      "|[32.0,12403.6,0.0...|    0|[1.09928017378346...|[0.90012015510305...|       0.0|\n",
      "|[32.0,13630.93,0....|    0|[1.23160744951434...|[0.92152247577356...|       0.0|\n",
      "|[33.0,11370.28,1....|    0|[1.30133493218139...|[0.93103321005047...|       0.0|\n",
      "|[33.0,12115.91,1....|    0|[1.09481928977197...|[0.89931518548223...|       0.0|\n",
      "|[33.0,12638.51,1....|    0|[1.24289779455747...|[0.92314001929513...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = y_pred.select(\"label\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8587360594795539"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Accuracy\n",
    "ac.filter(ac.label == ac.prediction).count()/ac.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbm.maxDepth, [2, 4, 6])\n",
    "             .addGrid(gbm.maxBins, [20, 30])\n",
    "             .addGrid(gbm.maxIter, [10, 20])\n",
    "             .build())\n",
    "\n",
    "cv = CrossValidator(estimator=gbm, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "cvModel = cv.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cvModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = y_pred.select(\"label\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8773234200743495"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac.filter(ac.label==ac.prediction).count()/ac.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We have increased the the accuracy of the model by using model tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's check the new customers will churn or not ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['names', 'age', 'total_purchase', 'account_manager', 'years',\n",
       "       'num_sites'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "names = pd.Series([\"Ali Ahmetolu\", \"Taner Gn\", \"Berkay Ozen\",\"Polat Konak\", \"Kamil Atasoy\"])\n",
    "age = pd.Series([38, 43, 34, 50, 40])\n",
    "total_purchase = pd.Series([30000, 10000, 6000, 30000, 100000])\n",
    "account_manager = pd.Series([1,0,0,1,1])\n",
    "years = pd.Series([20, 10, 3, 8, 30])\n",
    "num_sites = pd.Series([30,8,8,6,50])\n",
    "\n",
    "\n",
    "new_customer = pd.DataFrame({\n",
    "    'names':names,\n",
    "    'age': age,\n",
    "    'total_purchase': total_purchase,\n",
    "    'account_manager': account_manager ,\n",
    "    'years': years,\n",
    "    'num_sites': num_sites})\n",
    "\n",
    "new_customer.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the spark dataframe to predict ml results\n",
    "new_sdf = spark.createDataFrame(new_customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+--------------+---------------+-----+---------+\n",
      "|        names|age|total_purchase|account_manager|years|num_sites|\n",
      "+-------------+---+--------------+---------------+-----+---------+\n",
      "|Ali Ahmetolu| 38|         30000|              1|   20|       30|\n",
      "|    Taner Gn| 43|         10000|              0|   10|        8|\n",
      "|  Berkay Ozen| 34|          6000|              0|    3|        8|\n",
      "|  Polat Konak| 50|         30000|              1|    8|        6|\n",
      "| Kamil Atasoy| 40|        100000|              1|   30|       50|\n",
      "+-------------+---+--------------+---------------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need the transform the data for the understanding of the ml model\n",
    "new_customers = vectorAssembler.transform(new_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cvModel.transform(new_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|        names|prediction|\n",
      "+-------------+----------+\n",
      "|Ali Ahmetolu|       1.0|\n",
      "|    Taner Gn|       0.0|\n",
      "|  Berkay Ozen|       0.0|\n",
      "|  Polat Konak|       0.0|\n",
      "| Kamil Atasoy|       1.0|\n",
      "+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.select(\"names\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
